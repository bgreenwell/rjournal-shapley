---
title: Explaining Predictions with Shapley Values---An Introduction to the fastshap Package
author:
  - name: Brandon M. Greenwell
    affiliation: University of Cincinnati
    address:
    - 2925 Campus Green Dr
    - Cincinnati, OH 45221
    - United States of America
    - ORCiD---\href{https://orcid.org/0000-0002-8120-0084}{0000-0002-8120-0084}
    email:  greenwell.brandon@gmail.com
  - name: ...
    affiliation: University of Cincinnati
    address:
    - 2925 Campus Green Dr
    - Cincinnati, OH 45221
    - United States of America
    - ORCiD---\href{https://orcid.org/0000-0002-8120-0084}{0000-0002-8120-0084}
    email:  greenwell.brandon@gmail.com
abstract: >
  An abstract of less than 150 words.
preamble: |
  % Any extra LaTeX you need in the preamble
  \usepackage{booktabs}
  \usepackage[normalem]{ulem}
  \usepackage{float}
  \newfloat{algorithm}{hbt}{lop}
  \floatname{algorithm}{Algorithm}
output: rticles::rjournal_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "80%",
  fig.align = "center",
  fig.pos = "!htb",
  message = FALSE,
  warning = FALSE
)
```

### TODO:

* ~~Flesh out outline/section headers.~~
* Finish bar tab example (or switch to something better).
* Find a good place to talk about "true to the model" versus "true to the data": https://arxiv.org/pdf/2006.16234.pdf (I think this is important for motivating the SampleSHAP approximation, which relies on randomly permuting instance values.)
* Fill out KernalSHAP section.
* Find motivating example for **iml** package; maybe credit card default risk?
* Find motivating example for **fastshap** package; maybe Ames housing?
* Find motivating example of interfacing with **shap** via **reticulate**. Can probably lift from the **fastshap** vigenette here: https://bgreenwell.github.io/fastshap/articles/fastshap-vs-shap.html.


## Introduction

Introductory section which may include references in parentheses
[@R], or cite a reference such as @R in the text.


## Background

So what's a Shapley value? The Shapley value is the average marginal contribution of a \emph{player} across all possible \emph{coalitions} in a \emph{game}. In the context of statistical/machines learning,
\begin{description}

  \item[Game:] The prediction task for a single observation $x$.
  
  \item[Gain:] The prediction for $x$ minus the average prediction for all training observations.
  
  \item[Players] The feature values of $x$ that collaborate to receive the gain (i.e., predict a certain value).
  
\end{description}

In particular, the Shapley contribution of the $i$-th feature to an instance $x$ is defined as
\begin{equation}
\nonumber
\phi_i\left(x\right) = \frac{1}{p!} \sum_{\mathcal{O} \in \pi\left(p\right)} \left[\Delta Pre^i\left(\mathcal{O}\right) \cup \left\{i\right\} - Pre^i\left(\mathcal{O}\right)\right], \quad i = 1, 2, \dots, p,
\end{equation}
where ...

A simple example may help clarify the main ideas.

### Fairly splitting a bar tab

Alex, Brad, and Brandon decide to go out for drinks after work. They shared a few pitchers of beer, but nobody payed attention to how much each person drank. What's a fair way to split the tab? Suppose we knew the follow information, perhaps based on historical data:
\begin{itemize}

  \item If Alex drank alone, he'd only pay \$10.
  
  \item If Brad drank alone, he'd only pay \$20.
  
  \item If Brandon drank alone, he'd only pay \$10.
  
  \item If Alex and Brad drank together, they'd only pay \$25.
  
  \item If Alex and Brandon drank together, they'd only pay \$15.
  
  \item If Brad and Brandon drank together, they'd only pay \$13.
  
  \item If Ales, Brad, and Brandon drank together, they'd only pay \$30.

\end{itemize}

**FIXME:** Finish later...

\begin{table}[]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
                      & \multicolumn{3}{l}{Marginal contribution} \\ \midrule
Permutation           & Alex        & Brad         & Brandon      \\ \midrule
Alex, Brad, Brandon   & \$10        & \$15         & \$5          \\ 
Alex, Brandon, Brad   & \$10        & \$15         & \$5          \\
Brad, Alex, Brandon   & \$5         & \$20         & \$5          \\
Brad, Brandon, Alex   & \$10        & \$20         & \$0          \\
Brandon, Alex, Brad   & \$5         & \$15         & \$10         \\
Brandon, Brad, Alex   & \$17        & \$3          & \$10         \\ \midrule
Shapley contribution: & \$9.50      & \$14.67      & \$5.83       \\ \bottomrule
\end{tabular}
\caption{Marginal contribution for each permutation of Alex, Brad, and Brandon (i.e., the order in which they arrive). The Shapley contribution is the average marginal contribution across all permutations. (Notice how each row sums to the total bill of \$30.)}
\end{table}

So the next time the bartender asks how you want to split the tab, whip out a pencil and do the math!


### Advantages

### Disadvantages


## Estimating Shapley values via Monte Carlo simulation: SampleSHAP

A single estimate of the contribution of $x_i$ to $f\left(x\right)$ is nothing the more than the difference between two predictions, where each prediction is based on a sort of Frankenstein instance that' are's constructed by swapping out values between the instance being explained ($x$) and an instance selected at random from the training data. To help stabilize the results, the procedure is repeated a large number, say, $R$, times, and the result averaged together. 

\begin{algorithm}
\begin{enumerate}
  \item For $j = 1, 2, \dots, R$:
  \begin{enumerate}
    \item Select a random permutation $\mathcal{O}$ of the sequence $1, 2, \dots, p$.
    \item Select a random instance $w$ from the training instances $\boldsymbol{X}$.
    \item Construct two new instances as follows:
    \begin{itemize}
      \item $b_1 = x$, but all the features in $\mathcal{O}$ that appear after feature $x_i$ get their values swapped with the corresponding values in $w$.
      \item $b_2 = x$, but feature $x_j$, as well as all the features in $\mathcal{O}$ that appear after $x_j$, get their values swapped with the corresponding values in $w$.
    \end{itemize}
    \item $\phi_{ij}\left(x\right) = f\left(b_1\right) - f\left(b_2\right)$.
  \end{enumerate}
  \item $\phi_i\left(x\right) = \sum_{j = 1} ^ R \phi_{ij}\left(x\right) / R$.
\end{enumerate}
\caption{Approximating the $i$-th feature's contribution to $f\left(x\right)$. \label{alg:SampleSHAP}}
\end{algorithm}

If there are $p$ features and $m$ instanced to be explained, this requires $2 \times R \times p \times m$ predictions (or calls to a scoring function). In practice, this can be quite computationally demanding, especially since $R$ needs to be large enough to produce good approximations to each $\phi_i\left(x\right)$. In practice, this depends on the variance of each feature in the observed training data, but typically $R >= 50--100$ will suffice (**Need reference**).


## Special cases

The following sections discuss two special cases where exact Shapley explanations can be computed efficiently: additive linear models, and shallow trees and tree ensembles.


### Linear models: LinearSHAP

**FIXME:** I believe these results assume independence between features. Need to check corresponding reference in https://arxiv.org/pdf/2006.16234.pdf.

**FIXME:** Cite somewhere \cite{strumbelj-2014-explaining}.

First, lets discuss how a feature's value contributes to a prediction $f\left(X\right)$ in a simple (additive) linear model. That is, let's assume for a moment that $f$ takes the form
\begin{equation}
\nonumber
  f\left(X\right) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p
\end{equation}

Recall that the contribution of the $i$-th feature to the prediction $f\left(X\right)$ is the difference between $f\left(X\right)$ and the expected prediction if the $i$-th featureâ€™s value were not known:
\begin{equation}
\nonumber
\begin{split}
  \phi_i\left(X\right) &= \beta_0 + \dots + \beta_i X_i + \dots + \beta_p X_p \\ &\quad\quad - \left(\beta_0 + \dots + \beta_i \E\left(X_i\right) + \dots + \beta_p X_p\right) \\
  &= \beta_i \left(X_i - \E\left(X_i\right)\right)
\end{split},
\end{equation}
where we estimate $\E\left(X_i\right)$ with the corresponding sample mean $\bar{X}_i$. The quantity $\phi_i\left(X\right)$ is also referred to as the \emph{situational importance of $X_i$} \citep{achen-1982-interpreting}.


### Tree-based models: TreeSHAP

**FIXME:** Need to find the right balance of details and complexity here. 


### Kernal-based approximate Shapley values: KernelSHAP

KernelSHAP \citep{lundberg-2017-KernelSHAP} uses a specially-weighted local linear regression to estimate SHAP values for any model. Unlike SampleSHAP...


## Shapley values in R (and other lnaguages)

Probably the first, and most widely used implementation of Shapley explanations is the Python \pkg{shap} library \citep{lundberg-2017-KernelSHAP}, which provides a Python implementation of SampleSHAP, KernelSHAP, TreeSHAP, and a few other model-specific Shapley methods (e.g., DeepSHAP, which is provides approximate Shapley values for deep learning models).

The \CRANpkg{iml} package \citep{R-iml} provides the \code{Shapley()} function, which is a direct implementation of Algorithm~\ref{alg:SampleSHAP}. It is written in \CRANpkg{R6} \citep{R-R6}.

Package \CRANpkg{iBreakDown} implements a general approach to explaining the predictions from supervised models, called \dfn{Break Down} \citep{gosiewska-2019-iBreakDown}. SampleSHAP values can be computed as a special case from random Break Down profiles; see \code{iBreakDown::\code{shap()} for details}. 

\CRANpkg{shapper} provides an R interface to the Python \pkg{shap} library using \CRANpkg{reticulate} \citep{R-reticulate}; however, it currently only supports KernelSHAP (\pkg{shap} itself supports SampleSHAP, TreeSHAP, LinearSHAP, as well as various other model-specific Shapley explanation methods).

I'm also aware of two experimental packages supporting Shapley explanations that are not currently on CRAN: \pkg{shapr} \citep{R-sellereite} and \pkg{shapFlex} \citep{R-shapFlex}. As previously discussed, one drawback of traditional Shapley values is the assumption of independent features (an assumption made by many IML procedures, in fact). To that end, the \pkg{shapr} package implements Shapley explanations that can account for the dependence between features \citep{aas-2019-explaining}, resulting in significantly more accurate approximations to the Shapley values. The package also includes an implementation of KernelSHAP that's consistent with the \pkg{shap} package for Python. The \pkg{shapFlex} package, short for Shapley flexibility, provides approximate Shapley values that incorporate causal constraints into the model's feature space, as described in \citet{frye-2019-asymmetric}.

TreeSHAP has been directly incorporated into most implementations of XGBoost \citep{chen-2016-xgboost} (including \CRANpkg{xgboost} \citep{R-xgboost}), CatBoost \citep{eronika-2017-catboost}, and LightGBM \citep{ke-2017-lightgbm}. Both \CRANpkg{fastshap} \citep{R-fastshap} and \CRANpkg{SHAPforxgboost} \citep{R-SHAPforxgboost} provide an interface to \pkg{xgboost}'s TreeSHAP implementation.

\CRANpkg{fastshap} provides an efficient implementation of SampleSHAP and makes it a viable option for explaining the predictions from model's where efficient model-specific Shapley methods do not exist or are not yet implemented.

In Julia, there's \pkg{SampleSHAP.jl}, which is a lightweight port of \pkg{fastshap}; \pkg{ShapML.jl}, which is another Julia implementation of SampleSHAP; and \pkg{ShapleyValues.jl}, which hasn't been updated since 2016.

The next two sections illustrate more in-depth use of the \pkg{iml} and \pkg{fastshap} packages, respectively.


### `iml::Shapley()`

### `fastshap::explain()`

<!-- Make sure PDPs and ICE curves are described prior to this section. -->

Like many post-hoc interpretation techniques (e.g., PDPs and ICE curves), SampleSHAP can be made more efficient by generating all the data up front, and scoring it only once (or twice, in the case of SampleSHAP). For example, PDPs and ICE curves can be efficiently constructed with only a single call to a scoring function by generating all of the required data up front using a single cross-join operation (which can be done rather efficiently in SQL or Spark). The scored data can then be post-processed/aggregated and displayed as either a PDP or set of ICE curves. An example using Spark with \CRANpkg{sparklyr} \citet{R-sparklyr} can be found here: \url{https://github.com/bgreenwell/pdp/issues/97}.

Fortunately, a similar trick can be exploited for SampleSHAP. Whether explaining a single instance with a large value of Monte Carlo reps ($R$), or explaining a large number of instances, the basic idea is to generate all the required Frankenstein instances $b_1$ and $b_2$ upfront, and stored in matrices $\boldsymbol{B}_1$ and $\boldsymbol{B}_2$, respectively. 

For example, suppose we wanted to estimate the contribution of $x_i$ for each of the $N$ rows of the available training data $\boldsymbol{X}$ using a single Monte-Carlo repetition in Algorithm~\ref{alg:SampleSHAP} (i.e., $R = 1$)\footnote{The same idea also extends to explaining new instances.}. To start, we can generate the $N$ random instances at once and store them in an $N \times p$ matrix $\boldsymbol{W}$. Rather generating $N$ random permutations $\mathcal{O}$, and constructing $b_1$ and $b_2$ one at a time, the \pkg{fastshap} package uses C++---via \CRANpkg{Rcpp} \citep{R-Rcpp}---to efficiently generate an $N \times p$ logical matrix $\boldsymbol{\mathcal{O}}$, where $\boldsymbol{\mathcal{O}}_{kl} = 1$ if feature $x_l$ appears before feature $x_i$ in the $k$-th permutation, and $0$ otherwise. This logical matrix can then be used to logically subset $\boldsymbol{X}$ and $\boldsymbol{W}$ to more efficiently construct $\boldsymbol{B}_1$ and $\boldsymbol{B}_2$ in a single swoop. The matrices (or data frames) can then be each scored once, and the difference taken, to generate a single replication of $\phi_i\left(x\right)$ for each row of $\boldsymbol{X}$.

Suppose instead we want to estimate the contribution of $x_i$ for a single instance $x$, but using a large value of $R$ for accuracy. We could employ the same trick, but in this case $\boldsymbol{X}$ would refer to the $R \times p$ matrix, where each row is a copy of the instance $x$.

\pkg{fastshap} also uses efficient exact methods for the special cases described in Sections...

\pkg{fastshap} is faster at computing Shapley values for a single feature for a large number of instances (or a large value of $R$ for a single instance). But what about a large number of features? Fortunately, Algorithm~\ref{alg:SampleSHAP} can be trivially parallelized across features, and this is built into \pkg{fastshap}.


### Example: A simple benchmark comparison

This section provides a brief example comparing various implementations of Shapley values using [Kaggleâ€™s Titanic: Machine Learning from Disaster competition](https://www.kaggle.com/c/titanic). While the true focus of the competition is to use machine learning to create a model that predicts which passengers survived the Titanic shipwreck, weâ€™ll focus on explaining predictions from a simple logistic regression model.

To start, weâ€™ll load the data, which are conveniently available in the \CRANpkg{titanic} package \citep{R-titanic}, and do a little bit of cleaning.

```{r titanic-load}
# Read in the data and clean it up a bit
titanic <- titanic::titanic_train
features <- c(
  "Survived",  # passenger survival indicator
  "Pclass",    # passenger class
  "Sex",       # gender
  "Age",       # age
  "SibSp",     # number of siblings/spouses aboard
  "Parch",     # number of parents/children aboard
  "Fare",      # passenger fare
  "Embarked"   # port of embarkation
)
titanic <- titanic[, features]
titanic$Survived <- as.factor(titanic$Survived)
titanic <- na.omit(titanic)

# Data frame containing just the features
X <- subset(titanic, select = -Survived)
```

Next, weâ€™ll use the stats::glm() to fit a logistic regression model with only main effects (i.e., no tw-way interactions, etc.).

```{r titanic-glm}
fit <- glm(Survived ~ ., data = titanic, family = binomial)
```
Suppose we wanted to explain the predicted survival probability for a new passenger named Jack:
```{r titanic-jack}
jack <- data.frame(
  Pclass = 3,
  Sex = factor("male", levels = c("female", "male")),
  Age = 20,
  SibSp = 0,
  Parch = 0,
  Fare = 15,  # lower end of third-class ticket prices
  Embarked = factor("S", levels = c("", "C", "Q", "S"))
)
```
Our logistic regression model predicts that Jackâ€™s log-odds of survival is
```{r titanic-jack-predict}
predict(fit, newdata = jack)
```
Yikes, thatâ€™s equivalent to estimated 13.64% predicted probability of survival! With a baseline (i.e., average) survival rate of 40.62%, can we explain why the model predicts Jack to be much lower? Enter...Shapley values.

There is a growing number of R packages that provide Shapley explanations, the two most popular arguably being \pkg{iml} and \pkg{iBreakDown}. In this example, weâ€™ll compare those with \pkg{fastshap}.

To start, we need to define a few things (prediction wrapper, as well as both \pkg{iml}- and \pkg{iBreakDown}-related helpers).
```{r titanic-helpers}
# Prediction wrapper to compute predicted probability of survive
pfun <- function(object, newdata) {
  predict(object, newdata = newdata)
}

# DALEX-based helper for iBreakDown
explainer <- DALEX::explain(fit, data = X, y = titanic$Survived,                                             predict_function = pfun, verbose = FALSE)

# Helper for iml
predictor <- iml::Predictor$new(fit, data = titanic, y = "Survived",
                                predict.fun = pfun)
```
Next, we call each implementationâ€™s Shapley-related function to compute explanations for Jackâ€™s prediction using 100 Monte Carlo repetitions.
```{r titanic-jack-explanations, fig.width=9, fig.height=3, out.width="100%", fig.cap="TBD."}
# Compute explanations
set.seed(1039)  # for reproducibility
ex1 <- iBreakDown::shap(explainer, B = 100, new_observation = jack)
ex2 <- iml::Shapley$new(predictor, x.interest = jack, sample.size = 100)
ex3 <- fastshap::explain(fit, X = X, pred_wrapper = pfun, nsim = 100,
                         newdata = jack)

# Plot results
library(ggplot2)  # for `autoplot()` function
p3 <- plot(ex1) + ggtitle("iBreakDown")
p2 <- plot(ex2) + ggtitle("iml")
p1 <- autoplot(ex3, type = "contribution") + ggtitle("fastshap")
fastshap::grid.arrange(p1, p2, p3, nrow = 1)
```

Each package comes loaded with itâ€™s own bells and whistles (e.g., \pkg{iml} and \pkg{iBreakDown} have particularly fantastic visualizations). The main selling point of \pkg{fastshap} is speed! For example, all three packages (in fact, all general and practical implementations of Shapley values) use Algorithm~\ref{alg:SampleSHAP} which requires a large number of Monte Carlo repetitions to achieve accurate results. Below is a simple benchmark looking at the estimated time (in seconds) to explain Jackâ€™s prediction as a function of the number of Monte Carlo repetitions for each implementation. (Note that this comparison does not make use of \pkg{fastshap}'s feature-wise parallelization.)

```{r titanic-benchmark, fig.cap="Quick benchmark between three different implementations of SampleSHAP for explaining Jack's unfortunate prediction."}
nsims <- c(1, 5, 10, 25, 50, 75, seq(from = 100, to = 1000, by = 100))
times1 <- times2 <- times3 <- numeric(length(nsims))
set.seed(904)
for (i in seq_along(nsims)) {
  message("nsim = ", nsims[i], "...")
  times1[i] <- system.time({
    iBreakDown::shap(explainer, B = nsims[i], new_observation = jack)
  })["elapsed"]
  times2[i] <- system.time({
    iml::Shapley$new(predictor, x.interest = jack, sample.size = nsims[i])
  })["elapsed"]
  times3[i] <- system.time({
    fastshap::explain(fit, X = X, newdata = jack, pred_wrapper = pfun, 
                      nsim = nsims[i])
  })["elapsed"]
}
pal <- palette.colors(3, palette = "Okabe-Ito")  # colorblind friendly palette 
plot(nsims, times1, type = "b", xlab = "Number of Monte Carlo repetitions",
     ylab = "Time (in seconds)", las = 1, pch = 19, col = pal[1L],
     xlim = c(0, max(nsims)), ylim = c(0, max(times1, times2, times3)))
lines(nsims, times2, type = "b", pch = 19, col = pal[2L],)
lines(nsims, times3, type = "b", pch = 19, col = pal[3L],)
legend("topleft",
       legend = c("iBreakDown", "iml", "fastshap"),
       lty = 1, pch = 19, col = pal, inset = 0.02)
```
The message to be taken from Figure~\ref{fig:titanic-benchmark} is that \pkg{fastshap} scales incredibly well with $N$ or $R$, as long as the corresponding \code{predict()} method does.

Oh, and \pkg{fastshap} can produce instant (and exact) Shapley contributions for this example.

```{r titanic-exact}
fastshap::explain(fit, newdata = jack, exact = TRUE)  # ExactSHAP
fastshap::explain(fit, X = X, pred_wrapper = pfun, nsim = 10000,
                  newdata = jack)  # SampleSHAP
predict(fit, newdata = jack, type = "terms")  # ExactSHAP (base R)
```


## Example: Ames housing data

Use `fastshap::explain()` to explain predictions from a LightGBM model on the Ames housing data using both exact and approximate explanations (**Note:** there's a current PR that will give exact functionality for LightGBM models, which is now on CRAN).


## Example: default of credit card clients

Use `iml::Shapley()` to explain most extreme predictions.


## Example: An example of interfacing directly with shap via reticulate would be cool!

TBD.


## Summary

This file is only a basic article template. For full details of _The R Journal_ style and information on how to prepare your article for submission, see the [Instructions for Authors](https://journal.r-project.org/share/author-guide.pdf).

\bibliography{greenwell}
